{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build your own concordance\n",
    "\n",
    "It took 500 Dominican munks to write the first concordance of the Latin bible, and it took Rabbi Mordecai Nathan 10 years to write the first concordance of the Hebrew bible. With Python, it only takes a matter of seconds to find words in a text, along with the surrounding words.\n",
    "\n",
    "Run each cell in this notebook one at a time, in order. If something in one cell doesn't work right, it might be because you have overwritten a variable, so try going back and running all the previous cells again.\n",
    "\n",
    "First run the code and check that everything works. Then, try modifying the code. Start with the first challenges, and then continue in order. Feel free to work together, and see how far you can get. The important thing is to learn, not to solve all the challenges!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: click in /usr/lib/python3/dist-packages (from nltk) (8.1.6)\n",
      "Collecting joblib (from nltk)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2024.9.11-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Collecting tqdm (from nltk)\n",
      "  Downloading tqdm-4.66.5-py3-none-any.whl.metadata (57 kB)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.9.11-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (797 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m797.0/797.0 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Downloading tqdm-4.66.5-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, regex, joblib, nltk\n",
      "Successfully installed joblib-1.4.2 nltk-3.9.1 regex-2024.9.11 tqdm-4.66.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# install the natural language toolkit package (nltk), which has a copy of several texts, \n",
    "#including the King James Bible\n",
    "\n",
    "%pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to /home/ucloud/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/gutenberg.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the nltk package so that it is accessible to Python, and download a collection of texts from Project Gutenberg\n",
    "import nltk\n",
    "nltk.download('gutenberg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a variable called \"bible\" which contains the text of the King James bible.\n",
    "bible = nltk.corpus.gutenberg.raw('bible-kjv.txt')\n",
    "\n",
    "# make all characters lowercase\n",
    "bible = bible.lower()\n",
    "\n",
    "# remove the \"\\n\" characters, which indicate line breaks in the text (newlines)\n",
    "bible = bible.replace('\\n', ' ')\n",
    "\n",
    "# split up the text into a long list of individual words\n",
    "bible = bible.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a variable called \"concordance\", and fill it with every occurrence of the phrase \"this world\", and a few words preceeding and following \"this world\"\n",
    "concordance = []\n",
    "for i, val in enumerate(bible):\n",
    "    if val == \"world\":\n",
    "        if bible[i-1] == \"this\":\n",
    "            concordance.append(str(' '.join(bible[i-5:i+5])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a look at what the algorithm has found\n",
    "# concordance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see how many instances of the phrase \"this world\" were found\n",
    "# len(concordance)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try again, but this time let's just search for \"world\" by itself, not \"this world\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "concordance = []\n",
    "for i, val in enumerate(bible):\n",
    "    if val == \"world\":\n",
    "        concordance.append(str(' '.join(bible[i-5:i+5])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a look at what the algorithm has found\n",
    "# concordance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see how many instances of just the word \"world\" were found\n",
    "# len(concordance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, in the cell below, modify the code to search for a different word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add your modified code here and run the cell...\n",
    "concordance = []\n",
    "for i, val in enumerate(bible):\n",
    "    if val == \"god\":\n",
    "        concordance.append(str(' '.join(bible[i-5:i+5])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concordance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(concordance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The nltk package has the full text of several other classic books. You can see what they are called by running the command in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.corpus.gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your turn!\n",
    "\n",
    "Here are a some more things you can try. In each case, I have given you a little bit of starter code to get you going, but the cells will not run without some additional code from you.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in /home/ucloud/.local/lib/python3.12/site-packages (3.9.1)\n",
      "Requirement already satisfied: click in /usr/lib/python3/dist-packages (from nltk) (8.1.6)\n",
      "Requirement already satisfied: joblib in /home/ucloud/.local/lib/python3.12/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/ucloud/.local/lib/python3.12/site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in /home/ucloud/.local/lib/python3.12/site-packages (from nltk) (4.66.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to /home/ucloud/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setup for challenges\n",
    "%pip install nltk\n",
    "import nltk\n",
    "nltk.download('gutenberg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Challenge 1: build your own concordance\n",
    "\n",
    "Pick a different book and a different word, or pair of words. Copy and paste from the code above to write some Python code that searches the book of your choice for the word or pair of words of your choice. Put this code in the cell below. By the way, some of the texts use the characters \"\\r\" for \"carriage return\" instead of \"\\n\" for \"newline\". You can remove these the same way that you remove the \"\\n\" characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add your code to search for a word or pair of words in a different book here\n",
    "\n",
    "# Create a variable called \"paradise\" which contains the text of Milton's Paradise Lost.\n",
    "paradise = nltk.corpus.gutenberg.raw('milton-paradise.txt')\n",
    "\n",
    "# make all characters lowercase\n",
    "paradise = paradise.lower()\n",
    "\n",
    "# remove the \"\\n\" characters, which indicate line breaks in the text (newlines)\n",
    "paradise = paradise.replace('\\n', ' ')\n",
    "\n",
    "# split up the text into a long list of individual words\n",
    "paradise = paradise.split(' ')\n",
    "\n",
    "# From here, this is referred to as text preprocessing:\n",
    "\n",
    "# Make concordance list, which counts amount of times the word \"satan\" appears in the book:\n",
    "par_concordance = []\n",
    "for i, val in enumerate(paradise):\n",
    "    if val == \"satan\":\n",
    "        par_concordance.append(str(' '.join(paradise[i-5:i+5])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hell?\"    so satan spake; and him beelzebub',\n",
       " 'the high capital  of satan and his peers. their',\n",
       " 'barbaric pearl and gold,  satan exalted sat, by merit',\n",
       " 'when beelzebub perceived--than whom,  satan except, none higher sat--with',\n",
       " 'kingly crown had on.  satan was now at hand,',\n",
       " 'side,  incensed with indignation, satan stood  unterrified, and',\n",
       " 'forbore: then these to her satan returned:--   ',\n",
       " '  he ceased; and satan stayed not to reply,',\n",
       " 'less hostile din;  that satan with less toil, and',\n",
       " 'and the gulf between, and satan there   coasting',\n",
       " 'inroad of darkness old,  satan alighted walks:  a',\n",
       " 'bound the ocean wave.  satan from hence, now on',\n",
       " \"o'er sea and land: him satan thus accosts.  uriel,\",\n",
       " 'that steep savage hill  satan had journeyed on, pensive',\n",
       " 'usher evening rose:  when satan still in gaze, as',\n",
       " 'bliss!  to whom thus satan with contemptuous brow. ',\n",
       " 'judge of wise  since satan fell, whom folly overthrew,',\n",
       " ' so threatened he; but satan to no threats ',\n",
       " 'north  they came; and satan to his royal seat',\n",
       " ' the banded powers of satan hasting on  with',\n",
       " 'his right side:  then satan first knew pain, ',\n",
       " 'on the other part,  satan with his rebellious disappeared,',\n",
       " ' whereto with look composed satan replied.  not uninvented',\n",
       " ' they worse abhorred.  satan beheld their plight, ',\n",
       " 'and just,  hindered not satan to attempt the mind',\n",
       " 'god at last  to satan first in sin his',\n",
       " 'mary, second eve,  saw satan fall, like lightning, down',\n",
       " 'that new world,  where satan now prevails; a monument',\n",
       " 'following the track  of satan to the self-same place',\n",
       " 'be the race  of satan (for i glory in',\n",
       " 'suffered.  the other way satan went down  the',\n",
       " ' wide open and unguarded, satan passed,  and all',\n",
       " 'of that bright star to satan paragoned;  there kept',\n",
       " 'thus began.  second of satan sprung, all-conquering death! ',\n",
       " 'example and future;  to satan only like both crime',\n",
       " 'the father, to dissolve  satan with his perverted world;']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Output the list of the chosen word + the 5 words in either direction from it\n",
    "par_concordance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " This concordance has 36 entries\n"
     ]
    }
   ],
   "source": [
    "# Output is the total amount of entries in the list made legible with an f-string\n",
    "print(f' This concordance has {len(par_concordance)} entries')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 2: compare lengths of books\n",
    "\n",
    "We can use the command `len` to find how many items there are in a list. E.g., to find the number of words in the list called `bible`, above, we can write: `len(bible)`. \n",
    "\n",
    "Use the starter code below to find out which book in the books included in `nltk` has the most words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The book austen-emma.txt has 164457 words\n",
      "The book austen-persuasion.txt has 86270 words\n",
      "The book austen-sense.txt has 123514 words\n",
      "The book bible-kjv.txt has 848001 words\n",
      "The book blake-poems.txt has 8886 words\n",
      "The book bryant-stories.txt has 49404 words\n",
      "The book burgess-busterbrown.txt has 16305 words\n",
      "The book carroll-alice.txt has 28387 words\n",
      "The book chesterton-ball.txt has 86481 words\n",
      "The book chesterton-brown.txt has 80382 words\n",
      "The book chesterton-thursday.txt has 59297 words\n",
      "The book edgeworth-parents.txt has 177685 words\n",
      "The book melville-moby_dick.txt has 221023 words\n",
      "The book milton-paradise.txt has 91832 words\n",
      "The book shakespeare-caesar.txt has 23339 words\n",
      "The book shakespeare-hamlet.txt has 33477 words\n",
      "The book shakespeare-macbeth.txt has 20164 words\n",
      "The book whitman-leaves.txt has 138730 words\n"
     ]
    }
   ],
   "source": [
    "# solution 1: print all the titles and numbers of words\n",
    "# starter code:\n",
    "\n",
    "books = nltk.corpus.gutenberg.fileids()\n",
    "\n",
    "for title in books:\n",
    "\n",
    "    # Book preprocessing:\n",
    "    book = nltk.corpus.gutenberg.raw(title)\n",
    "    book = book.lower()\n",
    "    book = book.replace('\\n', ' ')\n",
    "    book = book.split(' ')\n",
    "\n",
    "    # Applying the length of the 'book' object to a new object called 'words':\n",
    "    words = len(book)\n",
    "    \n",
    "    # Print all book titles and their word count, made legible with an f-string:\n",
    "    print(f'The book {title} has {words} words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title of book and the amount of words: ('bible-kjv.txt', 848001)\n"
     ]
    }
   ],
   "source": [
    "# more advanced, for those with some Python experience, or those who want to google around..\n",
    "# solution 2: make a list of titles and a list of wordcounts, zip them together, then sort them based on wordcount\n",
    "# starter code:\n",
    "\n",
    "# Store Gutenberg files in the 'books' object\n",
    "books = nltk.corpus.gutenberg.fileids()\n",
    "\n",
    "# Make lists for future processing\n",
    "Book_wordcount = []\n",
    "titles = []\n",
    "words = []\n",
    "\n",
    "for title in books:\n",
    "\n",
    "    # Book preprocessing:\n",
    "    book = nltk.corpus.gutenberg.raw(title)\n",
    "    book = book.lower()\n",
    "    book = book.replace('\\n', ' ')\n",
    "    book = book.split(' ')\n",
    "\n",
    "    # Applying the length of the 'book' object to a new object called 'numwords':\n",
    "    words.append(len(book))\n",
    "    titles.append(title)\n",
    "\n",
    "# Zip together the 'title' list and the 'words' list into one zip-object:\n",
    "wordcount = zip(titles, words)\n",
    "\n",
    "# Sort the new list by the second column. Since it sorts automatically- \n",
    "# -from least to most, the reverse argument is used to make it from most to least\n",
    "sorted_wordcount = sorted(wordcount, reverse = True,\n",
    "                        key = lambda x: x[1])\n",
    "\n",
    "# Print only the first entry, thereby the book with the most words.\n",
    "print(f'Title of book and the amount of words: {sorted_wordcount [0]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 3: what are the most frequent words?\n",
    "\n",
    "`nltk` has a built-in function called `FreqDist` which counts up how many times each word in a text occurs. So, if you have a list called `words` which contains all the words in a book, you can find the frequencies of all of them by writing `freq = nltk.FreqDist(words)`. You can then get the e.g. ten most common words by writing `freq.most_common(10)`. What are the ten most common words in Jane Austen's \"Emma\"? What about Herman Melville's \"Moby Dick\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most frequent words in the books \"Emma\" and \"Paradise Lost\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None,\n",
       " [[('to', 4475),\n",
       "   ('the', 4380),\n",
       "   ('', 3704),\n",
       "   ('of', 3647),\n",
       "   ('and', 3357),\n",
       "   ('a', 2707),\n",
       "   ('I', 2151),\n",
       "   ('was', 2063),\n",
       "   ('not', 1808),\n",
       "   ('in', 1791)]],\n",
       " [[('and', 2704),\n",
       "   ('the', 2500),\n",
       "   ('to', 1752),\n",
       "   ('of', 1483),\n",
       "   ('', 1357),\n",
       "   ('in', 1080),\n",
       "   ('his', 981),\n",
       "   ('with', 871),\n",
       "   ('or', 562),\n",
       "   ('all', 553)]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# starter code:\n",
    "\n",
    "# Create lists for future storage:\n",
    "freqdist1 = []\n",
    "freqdist2 = []\n",
    "\n",
    "# First chosen book preprocessing:\n",
    "book1 = nltk.corpus.gutenberg.raw('austen-emma.txt')\n",
    "words1 = book1.lower()\n",
    "words1 = book1.replace('\\n', ' ')\n",
    "words1 = book1.replace('\\r', ' ')\n",
    "words1 = book1.split(' ')\n",
    "\n",
    "# Second book preprocessing:\n",
    "book2 = nltk.corpus.gutenberg.raw('milton-paradise.txt')\n",
    "words2 = book2.lower()\n",
    "words2 = book2.replace('\\n', ' ')\n",
    "words2 = book2.replace('\\r', ' ')\n",
    "words2 = book2.split(' ')\n",
    "\n",
    "# Make new object with most frequent words \n",
    "freq1 = nltk.FreqDist(words1)\n",
    "freq2 = nltk.FreqDist(words2)\n",
    "\n",
    "# Append frequent distribution to list to make output more legible:\n",
    "freqdist1.append(freq1.most_common(10))\n",
    "freqdist2.append(freq2.most_common(10))\n",
    "\n",
    "# Output specifying books worked with, and then list of 10 most common words in both:\n",
    "print(f'Most frequent words in the books \"Emma\" and \"Paradise Lost\"'), freqdist1, freqdist2\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 4: Remove stopwords\n",
    "\n",
    "Often, the most frequent words are not the most interesting ones. Words like \"a\" and \"the\" are so common in English, that they don't really tell us much about the text. That is why we often remove \"stopwords\", that is, a list of the most common words in English, before e.g. counting frequencies. There are several of these lists available, in [English]((https://gist.github.com/sebleier/554280)) as well as other languages, such as [Danish](https://gist.github.com/berteltorp/0cf8a0c7afea7f25ed754f24cfc2467b). Below is some starter code to remove stopwords. Use these snippets to see what the most common words in Emma and Moby Dick are after removing these most common words.\n",
    "\n",
    "Hint: In Moby Dick, you will also have to remove the string `\\r`, in addition to removing `\\n`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of most common words i \"Emma\" by Jane Austen, excluding stop words\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None,\n",
       " [[('I', 2151),\n",
       "   ('Mr.', 936),\n",
       "   ('could', 682),\n",
       "   ('would', 676),\n",
       "   ('Mrs.', 575),\n",
       "   ('Miss', 491),\n",
       "   ('must', 472),\n",
       "   ('much', 374),\n",
       "   ('said', 354),\n",
       "   ('every', 349)]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create list of stopwords:\n",
    "stopwords = [\"\", \"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\",\n",
    "             \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\",\n",
    "             \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\",\n",
    "             \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\",\n",
    "             \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\",\n",
    "             \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\",\n",
    "             \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\",\n",
    "             \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\",\n",
    "             \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\",\n",
    "             \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\",\n",
    "             \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\",\n",
    "             \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\",\n",
    "             \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"]\n",
    "\n",
    "freqdist = []\n",
    "book = nltk.corpus.gutenberg.raw('austen-emma.txt')\n",
    "\n",
    "# Book preprocessing:\n",
    "words = book.lower()\n",
    "words = book.replace('\\n', ' ')\n",
    "words = book.replace('\\r', ' ')\n",
    "words = book.split(' ')\n",
    "\n",
    "# code to exclude stopwords from wordcount:\n",
    "words = [word for word in words if word not in stopwords]\n",
    "\n",
    "# Make a frequence distribution of the list 'words':\n",
    "freq = nltk.FreqDist(words)\n",
    "\n",
    "# Append frequence distribution to new list to make output more legible\n",
    "freqdist.append(freq.most_common(10))\n",
    "\n",
    "# Output list of 10 most common words, excluding stop-words:\n",
    "print(f'List of most common words i \"Emma\" by Jane Austen, excluding stop words'), freqdist"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
